{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>The BYU Multiple AGent Intelligent Coordination and Control Lab, also known as the MAGICC Lab focuses on research involving navigation, guidance, and control of teams of autonomous Unmanned Aerial Vehicles (UAVs). The lab was founded by professors Randy Beard and Tim McLain in 1999 and is located in room 146 of the BYU Engineering Building.</p> <p></p> <p>Coordinated control of multiple vehicle teams is an active research area. The motivation for multiple vehicle teams is to achieve the same gains for mechanically controlled systems as has been gained in distributed computation. Rather than having a single monolithic (and therefore expensive and complicated) machine do everything, the hope is that many inexpensive, simple machines, can achieve the same, or enhanced functionality, through coordination. In essence, the objective is to replace expensive complicated hardware with coordination software. There are numerous applications for multiple vehicle teams including space-based interferometry, future autonomous combat systems, autonomous household items, enhanced surveillance, hazardous material handling, and active reconfigurable sensing. </p> <p>A significant early achievement of the lab is the development of the Kestrel autopilot and the development of algorithms to extend the cooperative and autonomous capabilities of the platform. The Kestrel autopilot was licensed to and sold by Procerus Technologies which was acquired by Lockheed Martin in 2012.</p>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#2025","title":"2025","text":""},{"location":"publications/#infrared-constellation-aided-landing-of-evtol-aircraft","title":"Infrared-Constellation-Aided Landing of eVTOL Aircraft","text":"<p>This paper presents an active fiducial light pattern localization (AFLPL) system that uses an IR camera and a constellation of IR lights to provide accurate pose estimates for eVTOL aircraft landing without GPS. When fused with an IMU via an EKF, the system provided sub-decimeter precision at ranges below 40 m and successfully demonstrated accurate tracking of a landing trajectory.</p>"},{"location":"publications/#safety-for-time-varying-parameterized-sets-using-control-barrier-function-methods","title":"Safety for Time-Varying Parameterized Sets Using Control Barrier Function Methods","text":"<p>This paper introduces a novel method for autonomous system safety that uses smooth Control Barrier Functions (CBFs) to keep entire sets of states\u2014not just single points\u2014away from unsafe areas. The technique leverages log-sum-exp functions to create smooth overapproximations of these sets, allowing safety-preserving controls to be computed via convex optimization, which was validated in multi-agent simulations.</p>"},{"location":"publications/#deep-learning-based-data-association-in-infrared-camera-images-for-gps-denied-aircraft-landing","title":"Deep Learning-Based Data Association in Infrared Camera Images for GPS-Denied Aircraft Landing","text":"<p>This paper presents a PointNet++ based deep learning approach for associating 2D infrared detections with 3D light sources for aircraft landing in GPS-denied environments. This method is robust to high clutter, operates in real-time, and was shown on flight-test data to produce pose estimates that outperform traditional GPS.</p>"},{"location":"publications/#non-uniform-b-spline-trajectory-optimization-using-control-point-representation-transformations","title":"Non-Uniform B-spline Trajectory Optimization using Control Point Representation Transformations","text":"<p>This paper uses non-uniform B-splines for UAV trajectory optimization, introducing transformations to MINVO and B\u00e9zier representations to leverage their tighter convex bounding properties. This approach yields trajectories that are significantly more time-optimal than uniform B-splines, though it comes with increased computational cost.</p>"},{"location":"publications/#vision-based-collision-avoidance-and-path-planning-for-uavs-using-bearing-and-pixel-area","title":"Vision-Based Collision Avoidance and Path Planning for UAVs Using Bearing and Pixel Area","text":"<p>This paper presents a UAV collision avoidance and path planning framework that uses only bearing and pixel area from a camera to predict object trajectories. This minimal-input system enables autonomous avoidance maneuvers with minimal ownship movement, improving safety in shared airspace.</p>"},{"location":"publications/#time-synchronized-b-spline-path-planning-for-multi-agent-uav-systems-with-fixed-speed-profiles","title":"Time-Synchronized B-Spline Path Planning for Multi-Agent UAV Systems with Fixed Speed Profiles","text":"<p>This paper proposes an offline path planning algorithm for multi-agent UAVs that must follow fixed speed profiles. By parameterizing uniform B-splines with a path variable to decouple geometry from speed, the method successfully ensures synchronized arrival times for coordinated missions, as shown in simulations.</p>"},{"location":"publications/#multiple-moving-object-detection-and-tracking-across-complex-terrain-and-skylines","title":"Multiple Moving Object Detection and Tracking Across Complex Terrain and Skylines","text":"<p>This paper presents a practical, non-machine learning method for tracking multiple moving objects in cluttered environments using low-cost cameras. It combines classic techniques like Kalman Filters, the Hungarian algorithm, and RANSAC to deliver robust, real-time tracking, even across challenging skylines.</p>"},{"location":"publications/#low-swap-gnss-denied-navigation-using-lte-signals-of-opportunity","title":"Low-SWaP GNSS-denied Navigation using LTE Signals of Opportunity","text":"<p>This paper develops a low-SWaP, GNSS-denied navigation technique for UAVs using high-power terrestrial LTE signals. The system uses a low-cost SDR to measure differential pseudoranges, which are fused with IMU data via an EKF, achieving an 8.03 m RMSE in a flight test.</p>"},{"location":"publications/#continuous-time-estimation-in-the-flat-output-space-using-b-splines","title":"Continuous-Time Estimation in the Flat Output Space Using B-Splines","text":"<p>This paper proposes a continuous-time trajectory estimation technique that operates in a system's differentially flat output space using B-splines, which naturally accounts for nonholonomic constraints. By first calibrating sensor-to-dynamics parameters, this method is shown in simulation to outperform traditional estimation on the configuration manifold.</p>"},{"location":"publications/#real-time-b-spline-path-planning-for-vision-based-collision-avoidance","title":"Real-Time B-Spline Path Planning for Vision-Based Collision Avoidance","text":"<p>This paper presents a real-time B-spline path planner for UAVs that uses only camera-derived bearing and pixel size for collision avoidance. This lightweight approach predicts obstacle trajectories and computes avoidance maneuvers, enabling autonomous decision-making in dynamic, shared airspaces.</p>"},{"location":"publications/#probabilistic-weapon-engagement-zones","title":"Probabilistic Weapon Engagement Zones","text":"<p>This paper introduces linearized probabilistic weapon engagement zones (PEZ), a method for planning safe trajectories in pursuer-evasion games while accounting for uncertainty in both friendly and adversary states. This approach effectively approximates the true risk distribution, allowing for path optimization in uncertain, adversarial environments.</p>"},{"location":"publications/#conflict-detection-resolution-and-control-of-aircraft-using-4d-polynomial-splines","title":"Conflict Detection, Resolution, and Control of Aircraft using 4D Polynomial Splines","text":"<p>This paper implements a distributed conflict detection and resolution algorithm for multiple quadrotors using 5<sup>th</sup>-order polynomial splines to define precise 4D (time-based) trajectories. The system, running on Raspberry Pi and Pixhawk hardware, successfully demonstrates that this flight path model can effectively separate vehicles.</p>"},{"location":"publications/#uncertainty-aware-velocity-obstacle-avoidance","title":"Uncertainty-Aware Velocity Obstacle Avoidance","text":"<p>This paper presents the uncertainty-aware velocity obstacle algorithm to ensure safe navigation for multiple aerial vehicles by incorporating position and velocity uncertainty. The 3D avoidance algorithm uses uncertainty bounds from an EKF to create a \"collision cone\" of unsafe areas, which was validated in both simulation and hardware experiments.</p>"},{"location":"publications/#distributed-conflict-detection-and-optimal-four-dimensional-trajectory-resolution-leveraging-polynomial-based-methods","title":"Distributed Conflict Detection and Optimal Four-Dimensional Trajectory Resolution Leveraging Polynomial-Based Methods","text":"<p>This paper presents a distributed conflict detection and resolution methodology using parametric fifth-order polynomial splines to define 4D aircraft trajectories. This representation allows for rapid conflict detection using Sturm sequencing and optimal resolution via gradient-based methods, as demonstrated in complex multi-aircraft simulations.</p>"},{"location":"publications/#optimization-of-4d-splines-for-unmanned-aerial-system-trajectories-under-kinematic-obstacle-and-time-constraints","title":"Optimization of 4D Splines for Unmanned Aerial System Trajectories Under Kinematic, Obstacle, and Time Constraints","text":"<p>This paper presents an optimization method for 4D 5<sup>th</sup>-order polynomial splines to plan safe, kinematically and time-constrained UAS trajectories around obstacles. The method shows these splines are a special subset of B'ezier splines, inheriting their convex hull properties, and numerical experiments validate the approach for navigating dynamic environments.</p>"},{"location":"publications/#external-lists","title":"External Lists","text":"<p>Dr. Tim McLain</p> <ul> <li>Publications</li> <li>Google Scholar</li> </ul> <p>Dr. Randy Beard</p> <ul> <li>Publications</li> <li>Google Scholar</li> </ul> <p>Dr. Cammy Peterson</p> <ul> <li>Publications</li> <li>Google Scholar</li> </ul> <p>Dr. James Usevitch</p> <ul> <li>Google Scholar</li> </ul>"},{"location":"directory/current_students/","title":"Current Students","text":""},{"location":"directory/current_students/#doctoral-students","title":"Doctoral Students","text":"<ul> <li> <p>Brandon Sutherland</p> </li> <li> <p></p> <p>David Akagi</p> </li> <li> <p></p> <p>Curtis Evans</p> </li> <li> <p></p> <p>Jen Jui Lui</p> </li> <li> <p></p> <p>Grant Stagg</p> </li> </ul>"},{"location":"directory/current_students/#masters-students","title":"Masters Students","text":"<ul> <li> <p>Jacob Moore</p> </li> <li> <p></p> <p>Ian Reid</p> </li> <li> <p></p> <p>Dean Anderson</p> </li> </ul>"},{"location":"directory/current_students/#undergraduate-students","title":"Undergraduate Students","text":""},{"location":"directory/faculty/","title":"Faculty","text":"<ul> <li> <p>Dr. Randy Beard</p> <p>Department: Electrical and Computer Engineering</p> <p>Email: beard@byu.edu</p> <p>Website: https://ece.byu.edu/directory/randy-beard</p> </li> <li> <p></p> <p>Dr. Tim McLain</p> <p>Department: Mechanical Engineering</p> <p>Email: mclain@byu.edu</p> <p>Website: https://www.me.byu.edu/faculty/timmclain</p> </li> <li> <p></p> <p>Dr. Cammy Peterson</p> <p>Department: Electrical and Computer Engineering</p> <p>Email: cammy.peterson@byu.edu</p> <p>Website: https://ece.byu.edu/directory/cammy-peterson</p> </li> <li> <p></p> <p>Dr. James Usevitch</p> <p>Department: Electrical and Computer Engineering</p> <p>Email: james_usevitch@byu.edu</p> <p>Websites:</p> <ul> <li>https://ece.byu.edu/directory/james-usevitch</li> <li>www.usevitch.com</li> </ul> </li> </ul>"},{"location":"directory/overview/","title":"Overview","text":"<p>Meet the faculty, current students, and alumni of the MAGICC Lab.</p> <p> </p>"},{"location":"directory/previous_students/","title":"Previous Students","text":""},{"location":"directory/previous_students/#2025","title":"2025","text":"<ul> <li> <p>Daniel Koch</p> <p>Degree: PhD Mechanical Engineering</p> </li> </ul>"},{"location":"directory/previous_students/#2024","title":"2024","text":"<ul> <li> <p>Aaron Brown</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p></p> <p>David Christensen</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p>Jaron Ellingson</p> <p>Degree: PhD Mechanical Engineering</p> </li> <li> <p>Coulton Karch</p> <p>Degree: MS Electrical Engineering</p> </li> </ul>"},{"location":"directory/previous_students/#2023","title":"2023","text":"<ul> <li> <p>Alex Jordan</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p></p> <p>Brendon Forsgren</p> <p>Degree: PhD Mechanical Engineering</p> </li> <li> <p></p> <p>Jake Johnson</p> <p>Degree: PhD Electrical and Computer Engineering</p> <p>Dissertation:  Continuous-time Trajectory Estimation and its Application to Sensor Calibration and Differentially Flat Systems</p> </li> </ul>"},{"location":"directory/previous_students/#2022","title":"2022","text":"<ul> <li> <p>Thane Downing</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Email: downing2@byu.edu</p> </li> <li> <p></p> <p>Tanner Norton</p> <p>Degree: MS Computer Science</p> <p>Email: tannort5@byu.edu</p> </li> <li> <p>Doug Graff</p> <p>Degree: MS Electrical and Computer Engineering</p> </li> </ul>"},{"location":"directory/previous_students/#2021","title":"2021","text":"<ul> <li> <p>Jacob Willis</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Thesis: \"Trajectory Generation and Tracking Control for Winged Electric Vertical Takeoff and Landing Aircraft,\"</p> <p>Email: jbwillis272@gmail.com</p> </li> <li> <p></p> <p>Matthew Rydalch</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Email: matthewk.rydalch@gmail.com</p> </li> <li> <p></p> <p>Hayden Morgan</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Thesis: \"Small-Target Detection and Observation with Vision-Enabled Fixed-Wing Unmanned Aircraft Systems,\" </p> <p>Email: haydenmmorgan@gmail.com</p> </li> <li> <p></p> <p>Mark Petersen</p> <p>Degree: PhD Electrical and Computer Engineering</p> <p>Dissertation: A Geometric Approach to Multiple Target Tracking Using Lie Groups</p> <p>Email: pet09034@byu.edu</p> </li> <li> <p></p> <p>Seth Nielsen</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Thesis: \"A Visually Realistic Simulator for Autonomous eVTOL Aircraft,\"</p> <p>Email: sethmnielsen@gmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2020","title":"2020","text":"<ul> <li> <p>Jared Moore</p> <p>Degree: MS Electrical and Computer Engineering</p> </li> </ul>"},{"location":"directory/previous_students/#2019","title":"2019","text":"<ul> <li> <p>Jerel Nielson</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Robust Visual-Inertial Navigation and Control of Fixed-Wing and Multirotor Aircraft</p> <p>Email: jerel.nielsen@gmail.com</p> </li> <li> <p>Jacob Olson</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p>Gary Ellingson</p> <p>Degree: PhD Mechanical Engineering</p> </li> <li> <p>James Jackson</p> <p>Degree: PhD Mechanical Engineering</p> </li> <li> <p>Michael Farrell</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p>Puneet Jain</p> <p>Degree: MS Electrical and Computer Engineering</p> </li> <li> <p>Skyler Tolman</p> <p>Degree: MS Electrical and Computer Engineering</p> <p>Thesis: \"Multiple Agent Target Tracking in GPS-Denied Environments,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2018","title":"2018","text":"<ul> <li> <p>Jesse Wynn</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: wynnsquad@gmail.com</p> </li> <li> <p></p> <p>Parker Lusk</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Path Planning for Unmanned Air and Ground Vehicles in Urban Environments,\"</p> <p>Email: parkerclusk@gmail.com</p> </li> <li> <p></p> <p>Jae Lee</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: Nonlinear Control Framework for Gimbal and Multirotor in Target tracking,</p> <p>Email: jaelee.byu@gmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2017","title":"2017","text":"<ul> <li> <p>Jeffrey Millard</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Multiple Target Tracking in Realistic Environments Using Recursive-RANSAC in a Data Fusion Framework,\"</p> <p>Email: jeffrey.d.millard@gmail.com</p> </li> <li> <p></p> <p>Matthew Duffield</p> <p>Degree: MS Mechanical Engineering and MBA</p> <p>Email: matt.o.duffield.bsm@gmail.com</p> </li> <li> <p></p> <p>David Wheeler</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Relative Navigation of Micro Air Vehicles in GPS-Degraded Environments</p> </li> <li> <p></p> <p>Jared Wikle</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: jkwikle@yahoo.com</p> </li> </ul>"},{"location":"directory/previous_students/#2016","title":"2016","text":"<ul> <li> <p>Matthew Argyle</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Modeling and Control of a Tailsitter with a Ducted Fan</p> <p>Email: matt.argyle@gmail.com</p> </li> <li> <p>Josh Sakamaki</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Cooperative Estimation for a Vision-Based Target Tracking System,\"</p> <p>Email: joshsakamaki@gmail.com</p> </li> <li> <p></p> <p>Laith Sahawneh</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Airborne Collision Detection and Avoidance for Small UAS Sense and Avoid Systems</p> <p>Email: laith20@hotmail.com</p> </li> <li> <p></p> <p>Benjamin Lewis</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"A Visual Return-to-Home System for GPS-Denied Flight,\"</p> <p>Email: benjamin.lewis.1000@gmail.com</p> </li> <li> <p>Jeff Ferrin</p> <p>Degree: PhD Mechanical Engineering</p> <p>Email: jeff_ferrin@hotmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2015","title":"2015","text":"<ul> <li> <p>Eric Quist</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: UAV Navigation and Radar Odometry</p> <p>Email: eric.quist@gmail.com</p> </li> <li> <p></p> <p>Paul Nyholm</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: pwnyholm@gmail.com</p> </li> <li> <p>Kyle Ingersoll</p> <p>Degree: BS Mechanical Engineering</p> <p>Email: jkiclimber@gmail.com</p> </li> <li> <p></p> <p>Patrick DeFranco</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: Detecting and Tracking Moving Objects from a Small Unmanned Air Vehicle, Email: pacadef@gmail.com</p> </li> <li> <p></p> <p>Dallin Briggs</p> <p>Degree: BS Mechanical Engineering</p> <p>Email: dallinbriggs@gmail.com</p> </li> <li> <p>James Ingersoll</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Vision Based Multiple Target Tracking Using Recursive RANSAC,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2014","title":"2014","text":"<ul> <li> <p>Jason Beach</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: jmbeach@byu.edu</p> </li> <li> <p></p> <p>Peter Niedfeldt</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Recursive-RANSAC: A Novel Algorithm for Tracking Multiple Targets in Clutter</p> <p>Email: pcniedfeldt@gmail.com</p> </li> <li> <p></p> <p>Justin Mackay</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: 1.732mackay@gmail.com</p> </li> <li> <p></p> <p>Nathan Edwards</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: nathanedwards8@gmail.com</p> </li> <li> <p></p> <p>Brandon Cannon</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: brandonjcan@gmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2013","title":"2013","text":"<ul> <li> <p>Joe Nichols</p> <p>Degree: PhD Mechanical Engineering</p> <p>Email: nichols@byu.edu</p> </li> <li> <p></p> <p>Robert Leishman</p> <p>Degree: PhD Mechanical Engineering</p> <p>Email: rleish@gmail.com</p> </li> <li> <p></p> <p>Robert Klaus</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: robertklaus@byu.net</p> </li> <li> <p></p> <p>Everett Bryan</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Cooperative Target Tracking Enhanced with the Sequence Memoizer,\"</p> <p>Email: eabryan@byu.edu</p> </li> <li> <p>Stephen Quebe</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Modeling, Parameter Estimation, and Gaze Control of Unmanned Indoor Quadrotor,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2012","title":"2012","text":"<ul> <li> <p>Ian Beaty</p> <p>Degree: BS Mechanical Engineering</p> <p>Email: grnhrnt@byu.net</p> </li> <li> <p></p> <p>Bryce Ready</p> <p>Degree: PhD Electrical Engineering</p> <p>Email: bryce.ready@gmail.com</p> </li> <li> <p></p> <p>Bryce Pincock</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Real-Time Target Following Using an Unmanned Rotorcraft with a Laser Rangefinder,\"</p> <p>Email: bpincock@yahoo.com</p> </li> <li> <p></p> <p>John Macdonald</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Efficient Estimation for Small Multi-Rotor Air Vehicles Operating in Unknown, Indoor Environments</p> <p>Email: johnatbyu@hotmail.com</p> </li> <li> <p></p> <p>Brandon Carroll</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Using Motion Fields to Estimate Video Utility and Detect GPS Spoofing,\"</p> <p>Email: ctnodnarb@yahoo.com</p> </li> <li> <p></p> <p>Liang (Solomon) Sun</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Dynamic Modeling, Trajectory Generation and Tracking, Simulations and Experiements of Aerially Towed Cable Systems for Aerial Recovery of Miniature Aerial Vehicles</p> <p>Email: sun.liang@byu.edu</p> </li> <li> <p>Kevin Meier</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Developing a Guidance Law for a Small-Scale Controllable Projectile Using Backstepping and Adaptive Control Techniques and a Hardware System Implementation for a UAV and a UGV to Track a Moving Ground Target,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2011","title":"2011","text":"<ul> <li> <p>Brett Millar</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: brett.millar@gmail.com</p> </li> <li> <p></p> <p>Rajnikant Sharma</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Bearing-only Cooperative Localization and Path Planning for Ground and Aerial Robots</p> <p>Email: rs535@et.byu.edu</p> </li> <li> <p>Huili Yu</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Vision-based Path Planning, Collision Avoidance, and Target Tracking for Unmanned Air and Ground Vehicles in Urban Environments</p> </li> <li> <p>Caleb Chamberlain</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"System Identification, State Estimation, and Control of Unmanned Aerial Robots,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2010","title":"2010","text":"<ul> <li> <p>Jacob Bishop</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: bishop.jacob@byu.edu</p> </li> <li> <p>Travis Millet</p> <p>Degree: MS Mechanical Engineering</p> </li> </ul>"},{"location":"directory/previous_students/#2009","title":"2009","text":"<ul> <li> <p>David Casbeer</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Decentralized Estimation Using Information Consensus Filters with a Multi-Static UAV Radar Tracking System</p> <p>Email: dwc8@email.byu.edu</p> </li> <li> <p></p> <p>Jeff Saunders</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Obstacle Avoidance, Visual Automatic Target Tracking, and Task Allocation for Small Unmanned Air Vehicles</p> <p>Email: saunders.jeff@gmail.com</p> </li> <li> <p></p> <p>Rhett Phillips</p> <p>Degree: BS Mechanical Engineering</p> <p>Email: rhetty2roll@hotmail.com</p> </li> <li> <p>Benjamin Heiner</p> <p>Degree: MS Electrical Engineering</p> <p>Email: benjamin.heiner@byu.net</p> </li> <li> <p></p> <p>James Hall</p> <p>Degree: PhD Mechanical Engineering</p> <p>Email: hallatkj@gmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2008","title":"2008","text":"<ul> <li> <p>Andrew Curtis</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Path Planning for Unmanned Air and Ground Vehicles in Urban Environments,\"</p> <p>Email: byudrew@gmail.com</p> </li> <li> <p></p> <p>Evan Andersen</p> <p>Degree: MS Electrical Engineering</p> <p>Email: andersen.evan@gmail.com</p> </li> <li> <p></p> <p>Jared Yates</p> <p>Degree: BS Electrical Engineering</p> </li> <li> <p></p> <p>Neil Johnson</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: neil_johnson@byu.edu</p> </li> <li> <p></p> <p>Steven Hansen</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: stevenhansen01@yahoo.com</p> </li> </ul>"},{"location":"directory/previous_students/#2007","title":"2007","text":"<ul> <li> <p>Blake Barber</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: d.blake.barber@gmail.com</p> </li> <li> <p></p> <p>Nathan Knoebel</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: nbk4@byu.edu</p> </li> <li> <p></p> <p>Derek Kingston</p> <p>Degree: PhD Electrical Engineering</p> <p>Thesis: \"Implementation Issues of Real-Time Trajectory Generation of Small UAVs,\"</p> <p>Dissertation: Decentralized Control of Multiple UAVs for Perimeter and Target Surveillance</p> <p>Email: derek_kingston@hotmail.com</p> </li> <li> <p></p> <p>Joe Jackson</p> <p>Degree: MS Mechanical Engineering</p> <p>Thesis: \"THESIS TITLE HERE\"</p> <p>Email: joseph_jackson@byu.edu</p> </li> <li> <p></p> <p>David Hubbard</p> <p>Degree: MS Computer Science</p> </li> <li> <p></p> <p>Joe Egbert</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Low-Altitude Road Following Using Strap-Down Cameras on Miniature Aerial Vehicles,\"</p> <p>Email: jegbert1@gmail.com</p> </li> <li> <p></p> <p>Justin Bradley</p> <p>Degree: MS Electrical Engineering</p> <p>Email: jmb275@et.byu.edu</p> </li> <li> <p></p> <p>Steve Osborne</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Transition Between Hover and Level Flight for a Tailsitter UAV,\"</p> <p>Email: srosborne@gmail.com</p> </li> <li> <p></p> <p>Jason Howlett</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Three Enabling Technologies for Vision-Based Forest-Fire Perimeter Surveillance Using Multiple Unmanned Aerial Systems,\"</p> <p>Email: holt.ryan@gmail.com</p> </li> </ul>"},{"location":"directory/previous_students/#2006","title":"2006","text":"<ul> <li> <p>Josh Matthews</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Adaptive Control of Micro Unmanned Air Vehicles,\"</p> </li> <li> <p></p> <p>David Johansen</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Video Stabilization and Object Localization using Feature Tracking with Small UAV Video,\"</p> <p>Email: davejohansen@gmail.com</p> </li> <li> <p></p> <p>Andrew Eldredge</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: andrewe@byu.edu</p> </li> <li> <p></p> <p>Brandon Call</p> <p>Degree: MS Mechanical Engineering J Thesis: \"Obstacle Avoidance for Unmanned Air Vehicles,\"</p> <p>Email: brandoncall@gmail.com</p> </li> <li> <p></p> <p>Jeff Anderson</p> <p>Degree: MS Electrical Engineering</p> <p>Email: jda064@gmail.com</p> </li> <li> <p>Jisang Sun</p> <p>Degree: MS Electrical Engineering</p> </li> <li> <p>Joshua Matthews</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Adaptive Control of Micro Unmanned Air Vehicles,\"</p> </li> </ul>"},{"location":"directory/previous_students/#2005","title":"2005","text":"<ul> <li> <p>Steve Griffiths</p> <p>Degree: MS Mechanical Engineering</p> </li> <li> <p></p> <p>Dale Rowley</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: dale.rowley@perceptekrobotics.com</p> </li> <li> <p></p> <p>Joshua Redding</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: jredding@byu.edu</p> </li> <li> <p></p> <p>Morgan Quigley</p> <p>Degree: BS Computer Science</p> <p>Email: morganquigley@gmail.com</p> </li> <li> <p></p> <p>Derek Nelson</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: derek.nelson@byu.net</p> </li> <li> <p></p> <p>Walt Johnson</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Design and Implementation of the Kestrel Autopilot,\"</p> <p>Email: waltj@procurusuav.com</p> </li> <li> <p></p> <p>Matt Blake</p> <p>Degree: MS Electrical Engineering</p> <p>Email: blakem@ee.byu.edu</p> </li> <li> <p>Gerrit Sorensen</p> <p>Degree: MS Electrical Engineering</p> <p>Email: gas4@email.byu.edu</p> </li> </ul>"},{"location":"directory/previous_students/#2004","title":"2004","text":"<ul> <li> <p>Jason Vest</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: jvest@et.byu.edu</p> </li> <li> <p></p> <p>Wei Ren</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation: Consensus Seeking, Formation Keeping, and Trajectory Tracking in Multiple Vehicle Cooperative Control</p> <p>Email: wren@engineering.usu.edu</p> </li> <li> <p></p> <p>Joshua Hintze</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Autonomous Landing of a Rotary Unmanned Aerial Vehicle in a Non-Cooperative Environment Using Machine Vision,\"</p> <p>Email: jh279@email.byu.edu</p> </li> <li> <p></p> <p>Reed Christiansen</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Design of an Autopilot for Small Unmanned Aerial Vehicles,\"</p> <p>Email: reedchristiansen@gmail.com</p> </li> <li> <p>David Walker</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: dhw6@et.byu.edu</p> </li> </ul>"},{"location":"directory/previous_students/#2003","title":"2003","text":"<ul> <li> <p>Steve Olson</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: olsonsa@ee.byu.edu</p> </li> </ul>"},{"location":"directory/previous_students/#2002","title":"2002","text":"<ul> <li> <p>Erik Anderson</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Extremal Control and Unmanned Air Vehicle Trajectory Generation,\"</p> <p>Email: eandersn@ee.byu.edu</p> </li> <li> <p></p> <p>Peter Jones</p> <p>Degree: BS Electrical Engineering</p> <p>Email: jonep@ee.byu.edu</p> </li> <li> <p></p> <p>Jason Howlett</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: howlett@et.byu.edu</p> </li> <li> <p>J. Willard Curtis</p> <p>Degree: PhD Mechanical Engineering</p> <p>Thesis: \"Nonlinear controller comparison on a benchmark system,\"</p> <p>Dissertation: Satisficing Control for Nonlinear Systems</p> </li> </ul>"},{"location":"directory/previous_students/#2001","title":"2001","text":"<ul> <li> <p>Jed Kelsey</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"The MAGICC Mobile Robot Toolbox (MMRT): A Simulink-based Control and Coordination Toolbox for Multiple Robotic Agents,\"</p> </li> <li> <p>Kevin Judd</p> <p>Degree: MS Electrical Engineering</p> <p>Email: kevjudd@us.ibm.com</p> </li> </ul>"},{"location":"directory/previous_students/#2000","title":"2000","text":"<ul> <li> <p>Brett Young</p> <p>Degree: MS Electrical Engineering</p> <p>Thesis: \"Mobile robots: coordination and control,\"</p> <p>Email: brett_j_young@west.raytheon.com</p> </li> <li> <p>Jonathan Lawton</p> <p>Degree: PhD Electrical Engineering</p> <p>Dissertation:  A behavior-based approach for spacecraft formation </p> <p>Email: jonathan_r_lawton@raytheon.com</p> </li> <li> <p>Chad Humberstone</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: chad@byu.net</p> </li> </ul>"},{"location":"directory/previous_students/#1999","title":"1999","text":"<ul> <li> <p>Chris Bailey</p> <p>Degree: MS Electrical Engineering</p> <p>Email: cbailey@orem.verio.net</p> </li> <li> <p>Tim Gold</p> <p>Degree: MS Mechanical Engineering</p> <p>Email: tgold@byu.net</p> </li> </ul>"},{"location":"directory/previous_students/#unknown","title":"Unknown","text":"<ul> <li> <p>Andres Rodriguez</p> <p>Degree: MS Electrical Engineering</p> <p>Email: andres.rodriguez@byu.edu</p> </li> <li> <p></p> <p>Brandon Reimschissel</p> <p>Degree: BS Electrical Engineering</p> <p>Email: brandon.reimschissel@gmail.com</p> </li> <li> <p>Sujit P.B.</p> <p>Degree: PhD Electrical Engineering</p> </li> </ul>"},{"location":"directory/students/brandon_sutherland/","title":"Brandon Sutherland","text":""},{"location":"directory/students/brandon_sutherland/#brandon-sutherland","title":"Brandon Sutherland","text":"<ul> <li>Email: bsuther2@byu.edu</li> <li>LinkedIn: https://www.linkedin.com/in/brandonsuth/</li> <li>GitHub: https://github.com/bsutherland333</li> </ul>"},{"location":"directory/students/brandon_sutherland/#about","title":"About","text":"<p>Brandon is a Doctoral student in the Electrical and Computer Engineering department with Dr. McLain as his advisor. He joined the MAGICC lab in Febuary 2021 and graduated from BYU with a Bachelor's degree in Mechanical Engineering in April 2024.</p> <p>Brandon first became interested in aerial robotics when he learned to build and fly FPV racing drones, where he began to obsess over how to get the fastest control response possible out of autopilot. He also enjoys mountain biking, building random things, and reading books in his spare time.</p>"},{"location":"directory/students/brandon_sutherland/#research","title":"Research","text":"<p>Brandon has been involved in a number of projects in the lab, most recently ROSflight as an undergraduate and now Cooperative GPS-Denied Navigation as a doctoral student. Brandon is currently working on non-invasive methods of adopting cooperative multi-agent navigation systems onto existing Kalman Filter based single-agent systems.</p> <ul> <li>Cooperative GPS Navigation</li> <li>ROSflight</li> </ul>"},{"location":"directory/students/brandon_sutherland/#papers","title":"Papers","text":"<ul> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft</li> </ul>"},{"location":"directory/students/curtis_evans/","title":"Curtis Evans","text":""},{"location":"directory/students/curtis_evans/#curtis-evans","title":"Curtis Evans","text":"<ul> <li>Email: cpe24@byu.edu</li> <li>LinkedIn : https://www.linkedin.com/in/curtis-evans-p/</li> <li>GitHub : https://github.com/curtispevans</li> </ul>"},{"location":"directory/students/curtis_evans/#about","title":"About","text":"<p>Curtis is a PhD student in the department of Electrical and Computer Engineering, with Randy Beard as his advisor. He received his undergraduate degree at BYU in mathematics with the Applied and Computational Mathematics Emphasis in 2024, and started researching in the MAGICC lab in 2024.</p> <p>When Curtis isn't working for the MAGICC lab he enjoys running and training for marathons, spending time with his family, and watching sports.</p>"},{"location":"directory/students/curtis_evans/#research","title":"Research","text":"<p>Curtis is working on the Sense and Avoid project.</p> <ul> <li>Sense and Avoid for Micro Aircraft Vehicles</li> </ul>"},{"location":"directory/students/curtis_evans/#papers","title":"Papers","text":"<ul> <li>Vision-Based Collision Avoidance and Path Planning for UAVs Using Bearing and Pixel Area</li> <li>Real-Time B-Spline Path Planning for Vision-Based Collision Avoidance</li> </ul>"},{"location":"directory/students/david_akagi/","title":"David Akagi","text":""},{"location":"directory/students/david_akagi/#david-akagi","title":"David Akagi","text":"<ul> <li>Email: dcakagi@byu.edu</li> <li>LinkedIn : https://www.linkedin.com/in/david-akagi-a7182211a</li> <li>GitHub : https://github.com/dcakagi</li> <li>Google Scholar : David Akagi</li> </ul>"},{"location":"directory/students/david_akagi/#about","title":"About","text":"<p>David is a PhD student in the Electrical and Computer Engineering Department advised by Dr. James Usevitch, exploring provably safe classical and learning-based control methods for autonomous systems. His prior master's work under Dr. Tim McLain focused on precision landing of eVTOL vehicles in GPS-denied environments. Outside of his research David loves to play sports, practice the piano, and catch up on watching One Piece with his wife.</p>"},{"location":"directory/students/david_akagi/#research","title":"Research","text":"<ul> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft</li> </ul>"},{"location":"directory/students/dean_anderson/","title":"Dean Anderson","text":""},{"location":"directory/students/dean_anderson/#dean-anderson","title":"Dean Anderson","text":"<ul> <li>Email: dben1182@student.byu.edu</li> <li>LinkedIn: https://www.linkedin.com/in/dean-a-1a1854103/</li> <li>GitHub: https://github.com/dben1182</li> </ul>"},{"location":"directory/students/dean_anderson/#about","title":"About","text":"<p>Dean Anderson is a Masters Student in the Brigham Young University Electrical Engineering department, with Dr. Randy Beard as his graduate advisor. He became a member of the MAGICC Lab in September 2023, at the beginning of his masters degree.</p>"},{"location":"directory/students/dean_anderson/#research","title":"Research","text":"<p>Dean is primarily involved in the current eVTOL Quadplane project; he focuses on path planning for obstacle avoidance and optimization for flight mode transition. He in involved in the lab's push for modular and repairable 3D printed airframes and is using one for his eVTOL project. He has also worked on aircraft system identification algorithms.</p>"},{"location":"directory/students/grant_stagg/","title":"Grant stagg","text":""},{"location":"directory/students/grant_stagg/#grant-stagg","title":"Grant Stagg","text":"<ul> <li>Email: ggs24@byu.edu</li> <li>Personal website</li> <li>LinkedIn</li> </ul>"},{"location":"directory/students/grant_stagg/#about","title":"About","text":"<p>Grant is a Doctoral student in the Electrical and Computer Engineering department at Brigham Young University, advised by Dr. Cameron Peterson. He earned his Bachelor\u2019s degree in Electrical Engineering from BYU in 2021.</p> <p>His research focuses on path planning for unmanned aerial vehicles (UAVs) in adversarial and uncertain environments, with an emphasis on algorithms that account for environmental uncertainty, sensor limitations, and intelligent adversaries. These methods aim to enable autonomous systems to operate more safely and effectively in challenging conditions.</p>"},{"location":"directory/students/grant_stagg/#research","title":"Research","text":"<p>Grant\u2019s research spans three core areas within UAV guidance and path planning:  </p> <ul> <li> <p>Probabilistic Engagement Zones (PEZs) \u2013 Develops mathematical models that allow UAVs to reason about threats from adversaries under uncertainty, characterizing the probability of engagement based on pursuer and evader capabilities to enable safer mission planning in contested environments.  </p> </li> <li> <p>Radar-Aware Path Planning \u2013 Creates trajectory optimization methods that both reduce uncertainty in radar parameters and avoid detection, addressing unknown radar characteristics such as location, power, and detection profile.  </p> </li> <li> <p>Hazard-Aware Path Planning \u2013 Designs algorithms for cooperative multi-agent exploration and mapping of environmental hazards, enabling UAV teams to efficiently locate and characterize hazardous regions in uncertain environments.</p> </li> </ul>"},{"location":"directory/students/grant_stagg/#projects","title":"Projects","text":"<p>Path Planning in Uncertain and Adversarial Environments</p>"},{"location":"directory/students/grant_stagg/#papers","title":"Papers","text":"<ul> <li>Bi-Level Route Optimization and Path Planning with Hazard Exploration \u2013 IEEE CDC 2025 (accepted)</li> <li>Turn-Rate Limited Probabilistic Weapon Engagement Zones \u2013 Under review</li> <li>Cooperative Multi-Agent Path Planning for Heterogeneous UAVs in Contested Environments \u2013 Under review</li> <li>Probabilistic Weapon Engagement Zones \u2013 ACC 2025</li> <li>Multi-Agent Path Planning for Level Set Estimation Using B-Splines and Differential Flatness \u2013 IEEE RAL 2024</li> <li>Decentralized Sparse Gaussian Process Regression with Event-Triggered Adaptive Inducing Points \u2013 Journal of Intelligent &amp; Robotic Systems 2023</li> </ul>"},{"location":"directory/students/ian_reid/","title":"Ian Reid","text":""},{"location":"directory/students/ian_reid/#ian-reid","title":"Ian Reid","text":"<ul> <li>Email: iyr27@byu.edu</li> <li>LinkedIn: https://www.linkedin.com/in/ian-y-reid/</li> <li>GitHub: https://github.com/iandareid</li> </ul>"},{"location":"directory/students/ian_reid/#about","title":"About","text":"<p>Ian Reid is a Electrical Engineering Master's student under Dr. McLain. He has been working in the MAGICC lab since 2022. </p> <p>He has an undergraduate in Mechanical Engineering from BYU. He is sucker for a good problem to solve no matter what it may be. Ian enjoy's carpentry, working on his ailing truck and is a space nerd fanatic. He has succesfully started and sold a business, and started another making a window washing drone and FAA remote ID modules.</p> <p>Ian loves to solve problems in sensible ways, looking for unexpected connections between problems and pushing the edge of research in autonomous systems.</p>"},{"location":"directory/students/ian_reid/#research","title":"Research","text":""},{"location":"directory/students/ian_reid/#current-projects","title":"Current Projects","text":"<p>Ian is researching state estimation, with a special focus on comparison between state estimation paradigms. </p> <p>He is currently working on ROSflight, a lean open source autopilot. It aims to be easily modifiable and understandable for reasearch applicaitons. He is also working on comparisons of cutting edge state estimation algorithms for multirotors, fixedwings and eVTOLs.</p> <ul> <li>ROSflight</li> </ul>"},{"location":"directory/students/ian_reid/#past-projects","title":"Past Projects","text":"<p>Ian has also worked on the GPS-denied landing of eVTOL aircraft using infrared light constellations.</p> <ul> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft</li> </ul>"},{"location":"directory/students/ian_reid/#papers","title":"Papers","text":"<ul> <li>FREDIM: Feasible Region Estimation and Decentralized Interception Method (ACC 2025)</li> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft (AIAA SciTech 2025)S</li> </ul>"},{"location":"directory/students/jacob_moore/","title":"Jacob Moore","text":""},{"location":"directory/students/jacob_moore/#jacob-moore","title":"Jacob Moore","text":"<ul> <li>Email: jmoore53@byu.edu</li> <li>LinkedIn: https://www.linkedin.com/in/jacob-moore-432761194/</li> <li>GitHub: https://github.com/JMoore5353</li> </ul>"},{"location":"directory/students/jacob_moore/#about","title":"About","text":"<p>Jacob is a master's student in the department of Electrical and Computer Engineering, with Dr. Tim McLain as his advisor. He received his undergraduate degree in Mechanical Engineering from BYU in April 2024, and has been immersed in aerial robotics since January 2022, when he started an internship for Teledyne FLIR. On the side, he has started several companies, including development of a window-washing drone and FAA-licensed Remote ID broadcast modules.</p> <p>Jacob is passionate about solving hard problems in robotics, flying drones, mechanical keyboards, and spending time with family.</p>"},{"location":"directory/students/jacob_moore/#research","title":"Research","text":""},{"location":"directory/students/jacob_moore/#current-projects","title":"Current Projects","text":"<p>Jacob is interested in enhancing robotic autonomy safely, especially in a multi-agent setting. A current focus is consistent and accurate localization and estimation.</p> <p>Current projects include ROSflight, an open-source autopilot aiming to streamline the research process, especially for eVTOL aircraft. He is also working on a project enhancing localization accuracy in GPS-denied regions even when communication is constrained.</p> <p>Please reach out with any questions you may have!</p> <ul> <li>ROSflight</li> <li>Cooperative GPS Navigation</li> </ul>"},{"location":"directory/students/jacob_moore/#past-projects","title":"Past Projects","text":"<p>Jacob assisted other research efforts including GPS-denied landing of an eVTOL aircraft with infrared beacons, precision-landing of a UAV on a moving boat, and interception of a fast-moving intruder with a swarm of slow-moving agents.</p> <ul> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft</li> <li>Robust Landing for UAVs in Maritime Environments</li> </ul>"},{"location":"directory/students/jacob_moore/#papers","title":"Papers","text":"<ul> <li>FREDIM: Feasible Region Estimation and Decentralized Interception Method (ACC 2025)</li> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft (AIAA SciTech 2025)</li> </ul>"},{"location":"directory/students/jen_jui_liu/","title":"Jen-Jui Liu","text":""},{"location":"directory/students/jen_jui_liu/#jen-jui-liu","title":"Jen-Jui Liu","text":"<ul> <li>Email: jjliu@byu.edu</li> <li>LinkedIn</li> <li>Google Scholar</li> <li>GitHub</li> </ul>"},{"location":"directory/students/jen_jui_liu/#about","title":"About","text":""},{"location":"directory/students/jen_jui_liu/#education","title":"Education","text":"<p>I\u2019m Jen\u2011Jui Liu, a Ph.D. candidate in Electrical &amp; Computer Engineering at Brigham Young University, advised by Prof. Randy Beard.</p>"},{"location":"directory/students/jen_jui_liu/#research-interests","title":"Research Interests","text":"<p>My research focuses on vision-guided UAV autonomy, with a particular emphasis on real-time path planning and collision avoidance in dynamic environments.</p>"},{"location":"directory/students/jen_jui_liu/#professional-background","title":"Professional Background","text":"<p>Before my doctoral studies, I worked as a Principal Engineer in the systems industry, where I developed expertise in machine learning and pattern recognition.</p>"},{"location":"directory/students/jen_jui_liu/#papers","title":"Papers","text":""},{"location":"directory/students/jen_jui_liu/#2025","title":"2025","text":"<ul> <li>Real-Time B-Spline Path Planning for Vision-Based Collision Avoidance - IEEE/ION Position, Location and Navigation Symposium (PLANS)</li> <li>Vision-Based Collision Avoidance and Path Planning for UAVs Using Bearing and Pixel Area - International Conference on Unmanned Aircraft Systems (ICUAS)</li> <li>Multiple Moving Object Detection and Tracking Across Complex Terrain and Skylines - Intermountain Engineering, Technology and Computing (IETC)</li> </ul>"},{"location":"directory/students/jen_jui_liu/#2024","title":"2024","text":"<ul> <li>Avoidance of Constant Velocity Targets Using Bearing and Time- to-Collision - American Control Conference (ACC)</li> </ul>"},{"location":"directory/students/jen_jui_liu/#2021","title":"2021","text":"<ul> <li>Using artificial intelligence to provide visual feedback for golf swing training - Electronic Imaging</li> <li>Automatic annotation of American Football Video footage for game strategy analysis - Electronic Imaging</li> </ul>"},{"location":"directory/students/jen_jui_liu/#2020","title":"2020","text":"<ul> <li>Body motion analysis for golf swing evaluation - International Symposium on Visual Computing</li> </ul>"},{"location":"directory/students/jen_jui_liu/#2010","title":"2010","text":"<ul> <li>Integrating photometric calibration with adaptive image halftoning for electrophoretic displays - Journal of Display Technology</li> <li>Design of flexible electrophoretic display controller with reduced waveform lookup tables - International Conference on Consumer Electronics</li> <li>Photometric calibration for image enhancement of electrophoretic displays - IEEE International Symposium on Consumer Electronics</li> </ul>"},{"location":"directory/students/jen_jui_liu/#2009","title":"2009","text":"<ul> <li>Automatic heart sound analysis with short-time fourier transform and support vector machines - IEEE International Midwest Symposium on Circuits and Systems</li> </ul>"},{"location":"research/current_projects/","title":"Current Projects","text":"<ul> <li> <p>Cooperative GPS Denied Navigation</p> <p>Cooperative navigation of heterogeneous robotic vehicles utilizing VIO, SLAM, and machine learning technologies.</p> <p></p> </li> <li> <p>ROSflight</p> <p>Streamlined, modular UAS autopilot software built specifically for researchers on a ROS2 framework.</p> <p></p> </li> <li> <p>Sense and Avoid for Micro Aircraft Vehicles</p> <p>Sense and avoid on a micro aircraft vehicle using bearing and pixel measurements from a camera mounted on the vehicle.</p> <p></p> </li> </ul>"},{"location":"research/past_projects/","title":"Past Projects","text":"<ul> <li> <p>Indoor Navigation</p> </li> <li> <p>Target Tracking</p> </li> <li> <p>GPS Denied Navigation</p> </li> <li> <p>Sense and Avoid for Unmanned Aircraft Systems</p> </li> <li> <p>Tailsitter</p> </li> <li> <p>Aerial Recovery</p> </li> <li> <p>Autopilot Design</p> </li> <li> <p>AUVSI Senior Project</p> </li> <li> <p>Commercialization</p> </li> <li> <p>Cooperative Control with Learning</p> </li> <li> <p>Fault Detection</p> </li> <li> <p>Cooperative Control</p> </li> <li> <p>Geo-referenced Mosaics</p> </li> <li> <p>Image-directed Control</p> </li> <li> <p>Path Planning and Trajectory Generation</p> </li> <li> <p>Tactical Seeability</p> </li> <li> <p>Infrared-Constellation-Aided Landing of eVTOL Aircraft</p> </li> <li> <p>Robust Landing for UAVs in Maritime Environments</p> </li> </ul>"},{"location":"research/projects/aerial_recovery/","title":"Aerial Recovery","text":""},{"location":"research/projects/aerial_recovery/#aerial-recovery","title":"Aerial Recovery","text":"<p>Aerial Recovery project studies the retrieval strategies for Micro Air Vehicles (MAVs) which are unable to return home by themselves. The basic concept is shown in the following figure. The mothership recovers a MAV by towing a long cable attached to a drogue. The drogue is actuated and maneuver and communicate with the MAV to facilitate successful capture. The MAV uses missile guidance strategies to intercept the drogue.</p>"},{"location":"research/projects/aerial_recovery/#drogue-orbit-regulation-by-mothership-orbit-maneuver","title":"Drogue Orbit Regulation by Mothership Orbit Maneuver","text":""},{"location":"research/projects/aerial_recovery/#given-a-desired-trajectory-of-the-drogue-a-motion-planning-strategy-based-on-differential-flatness-is-employed-to-calculate-the-desired-trajectory-for-the-mothership","title":"Given a desired trajectory of the drogue, a motion planning strategy based on differential flatness is employed to calculate the desired trajectory for the mothership.","text":""},{"location":"research/projects/aerial_recovery/#vision-based-rendezvous-of-mav","title":"Vision Based Rendezvous of MAV","text":""},{"location":"research/projects/aerial_recovery/#actuated-drogue-design","title":"Actuated Drogue Design","text":""},{"location":"research/projects/aerial_recovery/#flight-test","title":"Flight Test","text":""},{"location":"research/projects/aerial_recovery/#personnel","title":"Personnel","text":"<p>Dr. Randy Beard, Dallin Briggs, Daniel Carlson, Steve Carlson, Dr. Mark Colton, Scott Condie, Jeff Ferrin, Dr. John Hedengren, Dr. Tim McLain, Larry Moore, Joseph Nichols, Mark Owen, Liang Sun, Jesse Williams</p>"},{"location":"research/projects/aerial_recovery/#sponsor","title":"Sponsor","text":"<p>This research was supported by the Air Force Office of Scientific Research under STTR contract No. FA 9550-09-C-0102 to Procerus Technologies and Brigham Young University.</p>"},{"location":"research/projects/aerial_recovery/#project-duration","title":"Project Duration","text":"<p>2008.07-2012.08</p>"},{"location":"research/projects/aerial_recovery/#publications","title":"Publications","text":"<p>M. Colton, L. Sun, D. Carlson, and R. Beard, \u201cMulti-vehicle dynamics and control for aerial recovery of micro air vehicles\u201d, Int. J. Vehicle Autonomous Systems, vol. 9, pp. 78-107, 2011. PDF L. Sun and R. W. Beard, \u201cTowed-body trajectory tracking in aerial recovery of micro air vehicle in the presence of wind\u201d. San Francisco, CA, USA: American Control Conference, 2011, pp. 3209-3214. PDF</p> <p>D. C. Carlson and M. B. Colton, \u201cOut-of-plane orbit estimation and tracking for aerial recovery of micro air vehicles\u201d, in 2010 IEEE International Conference on Robotics and Automation, Anchorage, Alaska, USA, 2010. PDF</p> <p>L. Sun and R. W. Beard, \u201cTowed body altitude stabilization and states estimation in aerial recovery of micro air vehicles\u201d. Toronto, Ontario Canada: AIAA, Guidance, Navigation and Control Conference, August 2010. PDF</p> <p>L. Sun, R. W. Beard, and M. B. Colton, \u201cMotion planning and control for mothership-cable-drogue systems in aerial recovery of micro air vehicles\u201d. Baltimore, MD, USA: American Control Conference (ACC), 2010, pp. 2101-2106. PDF</p> <p>L. Sun, R. W. Beard, M. B. Colton, and T. W. McLain., \u201cDynamics and control of cable-drogue system in aerial recovery of micro air vehicles based on Gauss's principle\u201d. St. Louis, MO, USA: 2009 American Control Conference, June 2009, pp. 4729-4734. PDF</p> <p>M. A. Owen, J. W. Nichols, and M. B. Colton. \u201cCooperative aerial tracking and rendezvous along time-optimal 3-dimensional curves\u201d. Portland, OR, USA: AIAA, Guidance, Navigation and Control Conference, August 2011.</p> <p>Jeffrey Ferrin, Joseph Nichols, and Timothy McLain. \u201cDesign and control of a maneuverable towed aerial vehicle\u201d. Minneapolis, MN, USA: AIAA, Guidance, Navigation and Control Conference, August 2012.</p> <p>Joseph Nichols, Jeff Ferrin, Mark Owen, and Timothy McLain. \u201cVision-enhanced aerial rendezvous along elliptical paths\u201d. Minneapolis, MN, USA: AIAA, Guidance, Navigation and Control Conference, August 2012.</p> <p>Joseph Nichols and Liang Sun. \u201cAutonomous aerial rendezvous of small unmanned aircraft systems using a towed cable system\u201d. Fort Walton Beach, FL, USA: 43<sup>rd</sup> Annual Society of Flight Test Engineers Symposium, October 2012.</p>"},{"location":"research/projects/autopilot_design/","title":"Autopilot Design","text":""},{"location":"research/projects/autopilot_design/#autopilot-design","title":"Autopilot Design","text":"<p>The main objective of this research group has been to develop a working autopilot platform for mini-unmanned air vehicles (MUAV) that would allow for testing and development of commercial products.</p> <p>In recent years, a great deal of research has resulted in the Kestrel autopilot (KAP) which BYU uses for its various unmanned aircraft. The Kestrel uses a Rabbit Microprocessor which is programmed with all the code necessary to coordinate onboard electronics with the ground station. The autopilot is getting ever smaller, with the most recent version weighing just 16 grams and measuring less than 2 in X 2 in.</p> <p>The Kestrel autopilot has been developed into a commercial product available for purchase through Procerus small autopilot Technologies. BYU's MAGICC lab and senior capstone projects have been crucial in the development and production of these conveniently small autopilot chips that are being used by the Air Force.</p> <p>pda-demo-short_clip_big.jpg While the plane can be flown entirely by the autopilot, commands are sent to the plane through a ground station which consists of a laptop computer, a modem, and an optional video receiver. The laptop computer uses a software program called Virtual Cockpit to control the plane's path and to receive telemetry data from the plane. Virtual Cockpit has evolved into a robust program, capable of controlling multiple planes in a single console, and has even been adapted to be run off of a PDA (Portable Digital Assistant) as shown in the movie at right (Click to watch).</p> <p>Recently, the Magicc autopilot has increased its auto-takeoff and auto-land capabilities. This feature makes it possible for each plane to be autonomous from the beginning to the end, ending the need for RC control during UAV missions. Just throw the plane and click the mouse, and the Zagi flies its mission. The pictures below are links to videos that demonstrate the auto-takeoff and auto-land features.</p> <p>Currently, the autopilot is based off of PID feedback control loops and manual gain-tuning. However, Josh Matthews, a graduate student, has recently developed adaptive control algorithms to automatically tune gains and has incorporated this into the autopilot code. Adaptive control allows the planes to fly without having to specify each of the gains. This vastly diminishes prep time and gives the planes the capability to respond to instantaneous problems in real-time. The algorithms he is using allow planes to adapt to conditions such as a broken aileron or a shift in weight.</p>"},{"location":"research/projects/auvsi_senior_project/","title":"AUVSI Senior Project","text":""},{"location":"research/projects/auvsi_senior_project/#auvsi-senior-project","title":"AUVSI Senior Project","text":"<p>The ECEN department at Brigham Young University is introducing a new senior project beginning Winter 06. This project will be headed by Dr. Clark Taylor and Dr. Randy Beard from Electrical and Computer Engineering, Dr. Tim McLain of the Mechanical Engineering Department, and Dr. Mike Goodrich of the Computer Science Department. The primary goals that the students will strive to achieve are:</p> <ol> <li>Getting a model airplane to fly autonomously</li> <li>Planning a course to efficiently search a pre-determined area</li> <li>Identifying and geo-locating ground targets</li> <li>Dropping bombs (small fishing weights) on a target identified by the user in the video</li> <li>Landing on a ground target.</li> </ol> <p>This will all take place in a team environment where several students are working together to achieve the objectives of the competition. Some of the best students from the class will be selected to represent BYU at the AUVSI undergraduate UAV competition in June. The senior project will leverage off research work performed in the MAGICC and HCMI labs.</p>"},{"location":"research/projects/boatlanding/","title":"Robust Landing for UAVs in Maritime Environments","text":""},{"location":"research/projects/boatlanding/#robust-landing-for-uavs-in-maritime-environments","title":"Robust Landing for UAVs in Maritime Environments","text":"<p>Precision landing of UAVs is a topic of increasing interest, especially in maritime environments due to the potentially catastrophic effect of a failed landing and the naturally dynamic environment. In this project, we developed a method that enables precision landing of a UAV on a boat-like platform using 1) only RTK GPS and 2) RTK-GPS and vision-based localization.</p> <p>We conducted flight test results and showed that the RTK-only solution was able to achieve precision landings with an accuracy of 10 centimeters.</p>"},{"location":"research/projects/boatlanding/#sponsors","title":"Sponsors","text":"<ul> <li>Lawrence Livermore National Laboratory</li> </ul>"},{"location":"research/projects/boatlanding/#personnel","title":"Personnel","text":""},{"location":"research/projects/boatlanding/#students","title":"Students","text":"<ul> <li>Alex Jordan</li> <li>Matthew Rydalch</li> <li>Michael Williamson Tabango</li> <li>Landon Shumway</li> <li>Jacob Moore</li> </ul>"},{"location":"research/projects/boatlanding/#faculty","title":"Faculty","text":"<ul> <li>Tim McLain</li> </ul>"},{"location":"research/projects/boatlanding/#videos","title":"Videos","text":""},{"location":"research/projects/boatlanding/#papers","title":"Papers","text":"<ul> <li>Precision Maritime Localization and Landing with Real-time Kinematic GNSS</li> </ul>"},{"location":"research/projects/commercialization/","title":"Commercialization","text":""},{"location":"research/projects/commercialization/#commercialization","title":"Commercialization","text":"<p>BYU is the home to a Center of Excellence for Unmanned Vehicle Technology. Under the Center of Excellence funding, the MAGICC Lab seeks to develop technologies that can be marketed in the real world. Procerus Technologies licenses much of the MAGICC Lab's technology such as the Kestrel Autopilot and the Virtual Cockpit Software.</p>"},{"location":"research/projects/cooperative_control/","title":"Cooperative Control","text":""},{"location":"research/projects/cooperative_control/#cooperative-control","title":"Cooperative Control","text":"<p>The objective of this project is to develop coordination algorithms and architectures that facilitate coordinated timing issues for UAVs. In particular we are working the coordinated strike problem which is battlefield scenario where N UAVs are tasked to strike N targets simultaneously.</p> <p>The problem is complicated by the fact that the battle field contains numerous known threats and unknown pop-up threats. Key innovative technologies developed by the project include (1) a unique coordination architecture that encapsulates the information required for successful coordination into \"coordination variables\" and \"coordination architectures\", (2) path planning and trajectory generation techniques that are geared specifically to timing critical missions.</p> <p>We have conducted experiments using the coordination architecture with three UAVs very successfully. Current efforts focus on improving the trajectory tracking capabilities of the UAVs using a concept involving reachability regions, and improving the timing aspects of the experiments using consensus algorithms.</p>"},{"location":"research/projects/cooperative_control_with_learning/","title":"Cooperative Control with Learning","text":""},{"location":"research/projects/cooperative_control_with_learning/#cooperative-control-with-learning","title":"Cooperative Control with Learning","text":"<p>Unmanned aerial systems (UAS) are increasingly seen as a cornerstone in developing the future defense infrastructure. Because UASs are contributing members this infrastructure, it is critical that they collaborate efficiently as part of a larger manned-unmanned team. Successful cooperative control solutions are needed for greater autonomy and improved collaboration of unmanned systems. The difficulty with current cooperative control solutions is that they lack the ability to learn or to improve performance of mission objectives. Without the ability to learn, a UAS may not adapt to dynamically changing environments which may result in poor mission performance or even mission failure. Multi-agent learning has been studied in the computer science (CS) community for a long time. The learning approaches that have come out of the CS community may not be suitable for real world challenges since real applications require large state space models. Also, much of the existing work demonstrates learning capabilities with a limited number of cooperative agents. Our work seeks to solve these limitations by combining the multi-agent learning and the cooperative control fields.</p> <p></p> <p>We will develop a hierarchical learning framework to enhance intelligence of cooperative multi-agent systems. This framework, once fully developed, will be a positive step towards the envisioned manned-unmanned teams working together with confidence. To this end, we will develop a city-like environment for the UASs to learn and a baseline system for performance measurement. We will develop and test multi-agent cooperative control algorithms that help to maximize learning and improve mission performance. Finally, we will write low-level learning algorithms to account for mission objectives and high-level learning algorithms to ensure mission success.</p>"},{"location":"research/projects/cooperative_control_with_learning/#personnel","title":"Personnel","text":"<ul> <li>Dr. Randy Beard</li> <li>Dr. Kevin Seppi</li> <li>Everett Bryan</li> <li>Kevin Cook</li> </ul>"},{"location":"research/projects/cooperative_control_with_learning/#sponsor","title":"Sponsor","text":"<p>This project is funded by the UtopiaCompression Corporation and the AFRL Air Vehicles Directorate.</p>"},{"location":"research/projects/cooperative_control_with_learning/#project-duration","title":"Project Duration","text":"<p>May 2012 - December 2012</p>"},{"location":"research/projects/cooperative_control_with_learning/#publications","title":"Publications","text":"<p>Expected Publications - September 2012</p>"},{"location":"research/projects/cooperative_gps_denied_nav/","title":"Cooperative GPS-Denied Navigation","text":""},{"location":"research/projects/cooperative_gps_denied_nav/#cooperative-gps-denied-navigation","title":"Cooperative GPS-Denied Navigation","text":"<p>Localization can be difficult when GPS signals are corrupted or lost due to adversarial spoofing, jamming, or environmental conditions such as underground, underwater environments, or urban canyons. We are researching how multi-agent systems can be used to improve localization estimates by efficiently covering larger areas, reducing uncertainty through shared sensor data, and creating a comprehensive representation of unknown environments from various perspectives.</p> <p>This research is done in collaboration with the BYU FRoSt Lab.</p>"},{"location":"research/projects/cooperative_gps_denied_nav/#research-areas","title":"Research Areas","text":""},{"location":"research/projects/cooperative_gps_denied_nav/#1-multi-agent-localization-with-outlier-rejection","title":"1 - Multi-agent Localization with Outlier Rejection","text":"<p>The presence of outliers in sensor measurements can severely degrade localization estimates. Group-k consistent set maximization (GkCM) can be used to perform outlier rejection for lower degree-of-freedom (DoF) measurements, but is not as well studied as its pairwise counterpart. We do research in GkCM and also address outlier rejection for bearing-only measurements, with applications to acoustic, camera, RF, and signals of opportunity sensor measurements.</p>"},{"location":"research/projects/cooperative_gps_denied_nav/#2-multi-agent-localization-with-existing-single-agent-vehicles","title":"2 - Multi-agent Localization with Existing Single-agent Vehicles","text":"<p>Existing single-agent systems need a method of being incorporated into cooperative localization frameworks that fully utilizes the single-agent\u2019s existing estimator performance without degrading it or requiring significant modification of the filter. We are researching methods of easily integrating diverse multi-agent measurement types in extended Kalman filter estimators.</p>"},{"location":"research/projects/cooperative_gps_denied_nav/#3-semantic-mapping-and-multi-view-localization-using-heterogeneous-teams","title":"3 - Semantic Mapping and Multi-View Localization Using Heterogeneous Teams","text":"<p>A heterogeneous multi-agent team enables efficient mapping and cross-view localization. A semantic, hierarchical, and sparse map representation (3D Scene Graph) allows for data-efficient storage and transfer, supporting high-level intelligent robotic tasks and enabling robust cross-view localization.</p> <p></p>"},{"location":"research/projects/cooperative_gps_denied_nav/#sponsors","title":"Sponsors","text":"<ul> <li>Center for Autonomous Air Mobility &amp; Sensing</li> <li>Air Force Research Laboratory</li> </ul>"},{"location":"research/projects/cooperative_gps_denied_nav/#personnel","title":"Personnel","text":""},{"location":"research/projects/cooperative_gps_denied_nav/#students","title":"Students","text":"<ul> <li>Brendon Forsgren</li> <li>Chad Samuelson</li> <li>Kalliyan Velasco</li> <li>Brandon Sutherland</li> <li>Gabe Snow</li> <li>Jacob Moore</li> </ul>"},{"location":"research/projects/cooperative_gps_denied_nav/#faculty","title":"Faculty","text":"<ul> <li>Tim McLain</li> <li>Joshua Mangelson</li> </ul>"},{"location":"research/projects/cooperative_gps_denied_nav/#papers","title":"Papers","text":"<ul> <li>Group-k Consistent Measurement Set Maximization for Robust Outlier Detection</li> <li>Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments</li> </ul>"},{"location":"research/projects/fault_detection/","title":"Fault Detection","text":""},{"location":"research/projects/fault_detection/#fault-detection","title":"Fault Detection","text":"<p>In order for UAVs to gain access to the National Airspace System (NAS), and to be more reliable in general, they must be capable of greater self-awareness. The ability to detect whether sensors are functioning properly is an important part of this increased self-awareness. In the case of smaller UAVs, which are limited by size, weight and power (SWAP) constraints, it is not possible to carry redundant sensors onboard for simple voting scheme fault detection methods. The fault detection research taking place in the MAGICC Lab focuses on the development of a method that is capable of fault detection without recourse to redundant sensors.</p> <p>This method is currently under development and is being tested under two different scenarios. The first scenario is a height-above-ground (HAG) sensing scenario using a laser rangefinder. Other sensors available on the platform include a forward-looking RGB-D camera and an IMU. Using these two other sensors, faults in the laser rangefinder must be detected. Preliminary experimental results look promising and the method is being refined and will be implemented for real-time fault detection onboard a rotorcraft UAV. The second scenario is detecting GPS spoofing or faults using an RGB camera and an elevation map. An optical flow field can be computed from the video and compared against an idealized motion field computed from the motion of the aircraft and the elevation map. This could detect differences between actual motion and GPS readings when a spoofing attack begins. It also has the potential to detect differences in terrain if a spoofing attack has already succeeded in diverting the aircraft to a new location.</p>"},{"location":"research/projects/fault_detection/#personnel","title":"Personnel","text":"<ul> <li>Brandon Cannon</li> <li>Brandon Carroll</li> <li>Dr. Tim McLain</li> <li>Dr. Randy Beard</li> </ul>"},{"location":"research/projects/fault_detection/#sponsor","title":"Sponsor","text":"<p>Project Duration Publications</p>"},{"location":"research/projects/geo_referenced_mosaics/","title":"Geo-referenced Mosaics","text":""},{"location":"research/projects/geo_referenced_mosaics/#geo-referenced-mosaics","title":"Geo-referenced Mosaics","text":"<p>Miniature UAVs (MAVs) have recently become a popular and inexpensive way to collect visual data about an area. However, there are some significant challenges associated with this data. The small size and weight of an MAV make it very susceptible to atmospheric turbulence. This makes the video captured by an MAV shaky and difficult to watch. Another significant challenge occurs due to the limited field of view typical of MAV imagery. This makes it difficult for an operator to maintain context of what is being seen in the video transmitted from an MAV. For instance, when viewing MAV video, it is difficult to determine the relative or absolute location of features in view or even to determine which direction in the video is North.</p> <p>The usefulness of MAV imagery can be significantly improved by combining still images or video frames with position information from the MAV autopilot to create a geo-referenced mosaic. The mosaic presents everything the MAV has seen in one view and contains geographic information which allows any point in the view to be accurately geo-located. In addition, if the mosaic is created and updated in real-time, the operator can see live video on the mosaic with none of the shaking and limited field of view problems.</p>"},{"location":"research/projects/geo_referenced_mosaics/#example-of-raw-vs-mosaicked-videos","title":"Example of raw vs. mosaicked videos","text":"<p>These videos further demonstrate the advantages of mosaics over raw video:</p> <p>Raw video - Captured at 30 frames per second with 640x480 resolution</p> <p>Mosaicked video - Mosaic created at 10 frames per second on a laptop with a 1.8 GHz Pentium M processor</p>"},{"location":"research/projects/geo_referenced_mosaics/#other-example-mosaics","title":"Other example mosaics","text":""},{"location":"research/projects/geo_referenced_mosaics/#current-research","title":"Current Research","text":"<p>In addition to improving imagery display for a single MAV, geo-referenced mosiacs are an excellent way to combine video from multiple MAVs in one view. This allows an operator to monitor multiple MAVs at a single glance, while still being able to zoom in to get get a close-up look at any particular image. We have implemented and flight tested such a system using three MAVs. Some images captured during testing of the system are shown below.</p> <p>Mosaic and satellite image from a single MAV, zoomed in to show detail</p> <p>Mosaics from multiple MAV locations, overlayed on a satellite image, zoomed out to show context between locations.</p> <p>Currently we are implementing a system which allows users to track the progress of an MAV in real-time from anywhere in the world using the Internet and Google Earth\u2122. The MAV ground station receives video and telemetry from the MAV and process them to create a geo-referenced mosaic. MAV status information as well as mosaics are periodically uploaded to a webserver by accessing the internet via a commercial cellular network. The webserver, in turn, process the received information and uses it to update a Google Earth\u2122 overlay showing all of the collected imagery as well as the current position and status of the MAV.</p>"},{"location":"research/projects/gps_denied_navigation/","title":"GPS Denied Navigation","text":""},{"location":"research/projects/gps_denied_navigation/#gps-denied-navigation","title":"GPS Denied Navigation","text":"<p>GPS-Denied Navigation studies using onboard sensors to navigate autonomously even when traversing GPS Denied environments. As missions may occur anytime of the day or night, much of the research focuses on using Radar as a sensor for Micro Air Vehicles (MAVs).</p>"},{"location":"research/projects/gps_denied_navigation/#image-and-position-recognition-with-sar-imagery","title":"Image and Position Recognition with SAR Imagery","text":""},{"location":"research/projects/gps_denied_navigation/#using-radar-to-mitigate-imuaccelerometer-drift","title":"Using Radar to mitigate IMU/accelerometer drift","text":""},{"location":"research/projects/gps_denied_navigation/#personnel","title":"Personnel","text":"<p>Beard, Randy Quist, Eric</p>"},{"location":"research/projects/gps_denied_navigation/#sponsor","title":"Sponsor","text":"<p>This research was supported by the Air Force Office of Scientific Research under SBIR contract No. ? to ImSAR LLC. and Brigham Young University.</p>"},{"location":"research/projects/gps_denied_navigation/#project-duration","title":"Project Duration","text":"<p>2011.03-2014.03</p>"},{"location":"research/projects/image_directed_control/","title":"Image-directed Control","text":""},{"location":"research/projects/image_directed_control/#image-directed-control","title":"Image-directed Control","text":"<p>Image-directed control is the research field associated with using sensors that \"see\" and respond, similar to how humans and animals sense things with their eyes. This technology is not new, but the Magicc lab tries to use effective and innovative approaches to solve this problem.One example of image-directed control is Computer Vision. In fact, computer vision is an extensive field that focuses on making computers \"see\" by recognizing objects and responding. As part of the Mars Lander project, sponsored by NASA'a JPL (Jet Propulsion Laboratory), we have enabled our UAVs to sense a parachute deployed by a fellow UAV and follow it until the plane reaches the ground. At this point, the airborne plane continues to orbit over the fallen plane to aid in recovery.</p> <p>Another example of image-directed control is called Optic Flow. Currently, we are working with an Australian University which has experience in optic flow sensing in order to build our own optic-flow-capable UAVs. Optic flow tracks information \"flowing\" past a skinny camera, often with a small resolution line of pixels. By tracking how quickly information passes through the sensor, the plane can sense how far away it is from obstacles and can be programmed to avoid them. Obstacle avoidance is a major field in UAV research which is necessary to make them safe and marketable. Capabilities such as computer vision and optic flow make our UAVs smarter and more robust.</p> <p>One current of example of using image-directed control on real planes is demonstrated by the work of David Casbeer, a graduate student at BYU. His work shows how a team of UAVs can help firefighters track the growth of forest fires. First, upon discovery of a fire, the UAV team converges and begins to circle the perimeter of the fire.</p> <p>As the planes travel, they track how far they've gone and send this information to each other and to the ground station. This allows firefighters to know how quickly the fire is growing and keeps them from getting trapped by rapidly expanding flames. David's work includes modeling the fire growth in the computer and planning coordination routines for the planes in order to most effectively relay information. The movie clip to the left demonstrates the growing fire simulation and a single plane approaching it and beginning to circle.</p> <p>The planes use image-direction to track the perimeter of the fire. Therefore, this project includes many aspects of Magicc research: first, it relies on an effectively running autopilot system; second, it requires the coordination of multiple agents in a single project; and third, it requires image-directed capability in order to track the fire. This is one way in which UAVs can be an effective commercial product as well. The movie at right is a model of how the planes will coordinate their paths and communicate with one another.</p>"},{"location":"research/projects/indoor_navigation/","title":"Indoor Navigation","text":""},{"location":"research/projects/indoor_navigation/#indoor-navigation","title":"Indoor Navigation","text":"<p>The Indoor Navigation group of the BYU MAGICC Lab pursues research to enable a fully autonomous air vehicle to search an a priori unknown and complex environment (e.g. The post-earthquake Fukushima nuclear complex or Al Qaeda cave networks). This type of environment imposes significant challenges on the design of the vehicle. We cannot assume anything about the structure of the environment (e.g. vertical walls, flat floors, etc.). We are also denied access to outside navigation aids (e.g. GPS, or radio beacons). An appropriate vehicle will need to be relatively small and agile, and it will need to develop its own representation (i.e. map) of the environment online using its own sensor data.</p> <p> </p> <p>Several techniques and technologies, each with its own set of design tradeoffs, must be used to address these challenges. Navigating through an unknown environment without external references requires some type of simultaneous localization and mapping (SLAM) algorithm. The complex nature of the environment precludes popular sensors such as 2D scanning laser rangefinders; inherently 3D sensors such as stereo or RGB-D cameras must be used. Finally, any vehicle suitable for this environment will be constrained by its size, weight, and power (SWAP) limitations. Limited computational resources follow as a natural consequence.</p> <p>We acquired an eight camera motion capture system from Motion Analysis Corp. in the spring of 2010, and our efforts on this project began the following fall. We have adopted the MikroKopter family of multirotor helicopters as our aerial platform. These helicopters are agile, relatively inexpensive, and capable of carrying significant payload for their size. However, multirotor helicopters have undamped dynamics that require accurate estimates of states, especially velocity, be fed back frequently to the autonomous control. In the summer of 2011 we developed a state estimation scheme based on an improved dynamic model that produces accurate state estimates based primarily in high-rate IMU data. The estimates only require infrequent (approx. 5-10 Hz) updates from an exteroceptive sensing algorithm.</p> <p>We have adopted the Microsoft Kinect\u00ae as our preferred exteroceptive sensor. The Kinect provides an RGB image at 30 Hz along with depth data for each pixel measured by an IR emitter and receiver. In the Spring of 2011 and the Spring of 2012 we developed a visual odometry algorithm for the Kinect capable of producing relative position and orientation estimates at nearly 30 Hz. The visual odometry algorithm can be used along with saved images and a place recognition algorithm to develop a locally\u2011metric, globally\u2011topological SLAM map sufficient for persistent, repeatable navigation.</p> <p>We have recently added a Microstrain 3DM-GX3-15 IMU, which has helped to improve our estimation results considerably.</p> <p>Cortex3DGraphic.png IMG_1333.jpeg Xbox-360-Kinect-Standalone.png</p> <p> </p> <p>Our current efforts center on: - Refining the visual odometry algorithm; - Integrating the estimator with the real-time control and path planning of the vehicle; - Developing an innovative approach to back-end optimization that more efficiently creates a globally\u2011metric representation of the environment.</p> <p>Most published work on indoor navigation makes strong assumptions about the structure of the environment. We aim to relax these assumptions that others use to allow scanning laser rangefinders or simplified vision processing. The bulk of the literature also describes systems that depend on globally referenced states for navigation; our approach allows globally referenced information to be refined outside of the time critical navigation routines. Future work could involve a variety of tasks such as more sophisticated path planning (e.g. balancing competing mission requirements), higher level behaviors (e.g. interacting with objects in the environment), or cooperation between multiple vehicles.</p>"},{"location":"research/projects/indoor_navigation/#personnel","title":"Personnel","text":"<p>John Macdonald Robert Leishman Stephen Quebe</p>"},{"location":"research/projects/indoor_navigation/#sponsor","title":"Sponsor","text":"<p>This project is primarily funded through the Department of Defense SMART Scholarship program.</p>"},{"location":"research/projects/indoor_navigation/#project-duration","title":"Project Duration","text":"<p>Until we graduate</p>"},{"location":"research/projects/indoor_navigation/#publications","title":"Publications","text":"<p>R. Leishman, J. Macdonald, R. Beard, and T. McLain, \"Improved Use of Accelerometers in Estimating Quadrotor Attitude and Velcocity\" in review for publication in IEEE Robotics and Automation Magazine R. Leishman, J. Macdonald, T. McLain, and R. Beard, \"Relative Navigation and Control of a Hexacopter\" in Proc. IEEE Int. Conf. Robotics and Automation, St. Paul MN, 2012</p> <p>R. Leishman, J. Macdonald, S. Quebe, J. Ferrin, T. McLain, and R. Beard, \"Utilizing an Improved Rotorcraft Dynamic Model in State Estimation\" in Proc. IEEE Int. Intelligent Robots and Systems, San Fransisco, September 2011.</p> <p>J. Ferrin, R. Leishman, R. Beard, and T. McLain, \"Differential Flatness-Based Control of a Rotorcraft for Aggressive Maneuvers\" in Proc. IEEE Int. Intelligent Robots and Systems, San Fransisco, September 2011.</p>"},{"location":"research/projects/infrared_landing_evtol_aircraft/","title":"Infrared-Constellation-Aided Landing of eVTOL Aircraft","text":""},{"location":"research/projects/infrared_landing_evtol_aircraft/#infrared-constellation-aided-landing-of-evtol-aircraft","title":"Infrared-Constellation-Aided Landing of eVTOL Aircraft","text":"<p>Advanced air mobility (AAM) seeks to enhance the capabilities of manned and unmanned aircraft to improve quality of life. One focus of AAM research (and several companies) is to develop eVTOL air taxis to revolutionize urban transport. Safety is critical for such objectives. One critical component to ensure safety is accurate navigation during the landing phase, especially when operating in complex and congested urban environments.</p> <p>GPS signals are crucial for most navigation problems, due to the ability of GPS to provide highly accurate global position. Urban environments, however, often suffer from degraded GPS reception, due to multipathing. Thus, navigation without GPS or with intermittent or degraded GPS signal is important to ensure safety.</p> <p>In this project, we developed a method to use a constellation of infrared lights to aid the landing of an eVTOL aircraft (multirotor). We demonstrated the proposed method on real hardware, showcasing the ability of the method to safely land aircraft even when GPS is not available or intermittent.</p>"},{"location":"research/projects/infrared_landing_evtol_aircraft/#sponsors","title":"Sponsors","text":"<ul> <li>Archer Aviation</li> </ul>"},{"location":"research/projects/infrared_landing_evtol_aircraft/#personnel","title":"Personnel","text":""},{"location":"research/projects/infrared_landing_evtol_aircraft/#students","title":"Students","text":"<ul> <li>Aaron Brown</li> <li>David Akagi</li> <li>Brandon Sutherland</li> <li>Ian Reid</li> <li>Jacob Moore</li> </ul>"},{"location":"research/projects/infrared_landing_evtol_aircraft/#faculty","title":"Faculty","text":"<ul> <li>Tim McLain</li> </ul>"},{"location":"research/projects/infrared_landing_evtol_aircraft/#significant-results","title":"Significant Results","text":"<ul> <li>Developed a method for robust data association to detect the light constellation.</li> <li>Designed an EKF that fused relative pose information from detected light constellation.</li> <li>Successfully demonstrated safe landing of the multirotor.</li> </ul>"},{"location":"research/projects/infrared_landing_evtol_aircraft/#photos","title":"Photos","text":""},{"location":"research/projects/infrared_landing_evtol_aircraft/#papers","title":"Papers","text":"<ul> <li>Infrared-Constellation-Aided Landing of eVTOL Aircraft (AIAA SciTech 2025)</li> </ul>"},{"location":"research/projects/path_planning_and_trajectory_generation/","title":"Path Planning and Trajectory Generation","text":""},{"location":"research/projects/path_planning_and_trajectory_generation/#path-planning-and-trajectory-generation","title":"Path Planning and Trajectory Generation","text":"<p>The goal of the path planner is to generate a way-point path from the plane's current position to a goal using a terrain containing obstacles of various types. Our UAV path planner uses Rapidly-exploring Random Trees (http://msl.cs.uiuc.edu/rrt/) to explore the terrain and find a path to a goal.</p> <p>In the diagram at right (click to see large image), the blue cross is the position of the airplane and the magenta cross is the goal. The path that the planner chose is represented in magenta and the graph of possible paths is yellow.</p> <p>Brandon Call also used this path planner to plan paths for a mars rover. He has included several iterations of this application's graph growing in the sequence illustrated below. In this image, the white terrain represents obstacles while the dark colors are flat, drivable terrain. The yellow branches are the rapidly-exploring trees, and the best path is indicated by the pink line. After the path planner makes a way-point path, the trajectory generation makes the path smooth and flyable for the UAV. In the future, plans include plotting a rough initial path quickly and then refining the latter portions of the path while the plane begins to fly the initial portion.</p> <p>Sponsor Air Force Office of Scientific Research</p> <p>Duration September 2002 \u2013 September 2003</p> <p>Project Description Trajectory generation creates paths between specified points that can be realized by an unmanned air vehicle. Paths can be created that preserve straight-line path length, minimize flight time, or guarantee observation of a given area.</p> <p>These equations represent how an airplane reacts to heading change input. Trajectory generation deals with how to satisfy these physical constraints while still getting the airplane to fly along a specified path.</p> <p>Because airplanes physically cannot turn too sharply, only a small region can be reached by the airplane in the next instant of time. This adds complexity to the problem of traversing a path, especially when the path has many corners.</p> <p>This figure shows how the trajectory generation algorithm calculates when the next turn should begin. By fixing all of the circles to the minimum turning radius of the airplane, we can ensure that the trajectory is realizable.</p> <p>Here are three possible paths that the airplane could take if it\u2019s purpose was to come near the point at the end of the triangle. The top preserves straight-line path length, which is very useful when coordinating the timing of multiple airplanes.</p> <p>The BYU Magicc Laboratory has implemented this trajectory generation algorithm as a step in coordinating the timing and movements of teams of airplanes.</p>"},{"location":"research/projects/rosflight/","title":"ROSflight","text":""},{"location":"research/projects/rosflight/#rosflight","title":"ROSflight","text":"<p>ROSflight is a lightweight, open-source flight control firmware and software stack designed for research-oriented unmanned aerial vehicles (UAVs), particularly multirotors and fixed-wing aircraft. Its primary motivation is to provide a simple, well-documented, and easily modifiable platform for academic and experimental robotics applications, avoiding much of the complexity of autopilots like ArduPilot or PX4.</p> <p>Extensive documentation on the project can be found at the ROSflight website.</p>"},{"location":"research/projects/rosflight/#sponsors","title":"Sponsors","text":"<ul> <li>Center for Autonomous Air Mobility &amp; Sensing</li> <li>AeroVironment</li> </ul>"},{"location":"research/projects/rosflight/#personnel","title":"Personnel","text":""},{"location":"research/projects/rosflight/#students","title":"Students","text":"<ul> <li>Daniel Koch</li> <li>James Jackson</li> <li>Trey Hendrickson</li> <li>Ian Reid</li> <li>Brandon Sutherland</li> <li>Jacob Moore</li> <li>Joseph Ritchie</li> <li>Derek Ward</li> </ul>"},{"location":"research/projects/rosflight/#aerovironment","title":"AeroVironment","text":"<ul> <li>Phil Tokumaru</li> </ul>"},{"location":"research/projects/rosflight/#faculty","title":"Faculty","text":"<ul> <li>Tim McLain</li> <li>James Usevitch</li> </ul>"},{"location":"research/projects/rosflight/#papers","title":"Papers","text":"<ul> <li>ROSflight: A lightweight, inexpensive MAV research and development tool</li> <li>ROSplane: Fixed-wing autopilot for education and research</li> <li>ROSflight: A Lean Open-Source Research Autopilot</li> </ul>"},{"location":"research/projects/rosflight/#code","title":"Code","text":"<ul> <li>ROSflight GitHub</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/","title":"Sense and Avoid for Micro Aircraft Vehicles","text":""},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#sense-and-avoid-for-micro-aircraft-vehicles","title":"Sense and Avoid for Micro Aircraft Vehicles","text":"<p>Collision avoidance becomes a problem when an unmanned micro aircraft vehicle is sharing the same airspace as a manned aircraft. Utilizing radar on the vehicle can help give the unmanned aircraft bearing and range of the observed vehicle, but radars are heavy and expensive. Meanwhile, cameras are cheap and inexpensive, but they lack in a range measurement. The purpose of this project is to utilize cheap and lightweight cameras to plan safe paths around observed vehicles.</p> <p>This project has a few areas a research, the first being target tracking using computer vision, the second being collision avoidance path planning, and the third being range estimation with bearing and pixel measurements.</p>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#sponsors","title":"Sponsors","text":"<p>This project is funded by Utopia Compression Corporation</p> <ul> <li>Utopia Compression Corporation</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#personnel","title":"Personnel","text":""},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#students","title":"Students","text":"<ul> <li>Curtis Evans</li> <li>Jen Jui Liu</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#faculty","title":"Faculty","text":"<ul> <li>Dr. Randy Beard</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#significant-results","title":"Significant Results","text":"<ul> <li> <p>Tracking an anaconda drone while the camera is in a stationary location. The outputs of this tracking algorithm are essentially the bounding box which contains a bearing in the x and y direction and a pixel size, i.e. how big is the bounding box. These outputs are what is used in the path planning and range estimation. </p> </li> <li> <p>Real-time path planning in a 2D scenario using only bearing and pixel measurements. The video shows ownship avoiding the wedges, which is a collision free path. The wedge is an area of where the observed object is likely to be, so a successful avoidance of the wedge is a successful avoidance of the observed object. </p> </li> </ul>"},{"location":"research/projects/sense_and_avoid_for_micro_aircraft_vehicles/#papers-theses-and-presentations","title":"Papers, Theses, and Presentations","text":"<ul> <li>Real-Time B-Spline Path Planning for Vision-Based Collision Avoidance</li> <li>Multiple Moving Object Detection and Tracking Across Complex Terrain and Skylines</li> <li>Vision-Based Collision Avoidance and Path Planning for UAVs Using Bearing and Pixel Area</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/","title":"Sense and Avoid for Unmanned Aircraft Systems","text":""},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/#sense-and-avoid-for-unmanned-aircraft-systems","title":"Sense and Avoid for Unmanned Aircraft Systems","text":"<p>It is obvious that the effective use of Unmanned Aerial Vehicles (UAV) for civilian or military applications requires flight through various civilian, restricted, and military airspace classes. To ensure that UAVs do not disrupt other airspace users, they must be integrated into the current National Airspace System (NAS), through the development of detailed and comprehensive rules, procedures, policies, standards and regulation that allow for safe, accident-free aviation. However proving that UAVs have the capability to provide the Equivalent Level of Safety (ELOS) as manned aviation remains a challengeig issue. Since the beginning of manned flight the primary method for avoiding midair collisions has been a pilot's eyes. Even with the introduction of radio communication, radar, and air trac control (ATC), the FAA has maintained the requirement for a pilot to actively search the sky for other airspace users and maneuver to avoid collision. While creating a robust system to identify intruder aircraft is an integral piece of the SAA problem, the development of a Collision Avoidance System (CAS) is equally important. If UAVs are to be integrated seamlessly alongside piloted aircraft, they must react to collision threats in the same way as a human pilot. In any encounter scenarios that occurs, the UAV's avoidance maneuver must conform to the standard fight rules followed by piloted aircraft.This work will aim to develop and implement a collision avoidance approach based on a chain placed in a force field. The chain-based strategy was initially presented by MAGICC lab researchers to plan a path as a planner that models the UAV path using a series of connected waypoints. The waypoints serve as the links of a simulated chain. The chain is placed in a force-field generated using the gradient of a bounded differentiable reward function. This causes each link in the chain to move toward local maxima of the reward function. Other forces are applied to prevent UAV fight constraints from being violated. Since the path is represented using waypoints that are a fixed distance apart, it is easy to determine roughly where the UAV will be at any given time. This timing information can be used to prevent collisions and spread out paths when creating plans for multiple UAVs.</p> <p></p>"},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/#personnel","title":"Personnel","text":"<ul> <li>Dr. Randy Beard</li> <li>Dr. Tim Mclain</li> <li>Laith Sahawneh</li> <li>Robert Klaus</li> </ul>"},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/#sponsor","title":"Sponsor","text":"<p>This project is funded by Utopia Compression Corporation and Air Force Research Laboratory.</p>"},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/#project-duration","title":"Project Duration","text":"<p>January 2012 - December 2012</p>"},{"location":"research/projects/sense_and_avoid_for_unmanned_aircraft_systems/#publications","title":"Publications","text":"<p>[1] Sahawneh, Laith, Beard, Randal W., Avadhanam, Sharath, Bai, He. \"Chain-based Collision Avoidance for UAS Sense-and-Avoid Systems\", AIAA Guidance, Navigation, and Control Conference, Boston, August, 2013.</p>"},{"location":"research/projects/tactical_seeability/","title":"Tactical Seeability","text":""},{"location":"research/projects/tactical_seeability/#tactical-seeability","title":"Tactical Seeability","text":"<p>Unmanned Air Systems (UASs) are used to strategically position aerial sensors, such Electro-Optical or Infra-Red cameras, to perform surveillance, tracking, and other missions. The position of the UAS, the ground location where the sensor is pointing, the resolution of the sensor, and other parameters all affect the quality of the received sensor measurements. This research has two main topics: To study and model the quality of sensor video, and to use this model to autonomously plan a UAS flight path and sensor schedule that maximizes the UAS operators' ability to detect and identify objects of interest from the video stream.</p>"},{"location":"research/projects/tactical_seeability/#video-utility-metric","title":"Video Utility Metric","text":"<p>In the literature, there exists methods to estimate an operator's abiliity to perform detection, recognition, and identification (DRI) tasks when observing an image. The targeting task performance (TTP) metric developed by Volmerhaussen and Jacobs is the state of the art method to estimate the probability of an operator sucessfully performing DRI tasks. Unfortunately, there is no previously existing method to estimate the probability of an operator successfully performing DRI tasks when observing video. For this reason, we extend the TTP metric by developing the video TTP (VTTP) metric.</p> <p>When observing images, object size, resolution, and contrast with the background are significant factors in determining whether or not an operator wil find the desired object. These factors are all considered when calculating the TTP value. When observing video, at least two additional components are needed. Blur due to motion and where the object is located in the video frame both affect the probability of an operator detecting the object. The VTTP metric is calculating the TTP value of a hypothetical object at a terrain point within the video, and then scaling the TTP value based on the amount of motion.</p>"},{"location":"research/projects/tactical_seeability/#project-members","title":"Project Members","text":"<p>Dr. Randy Beard, Dr. Bryan Morse, Dr. Stephen Pledgie, Peter Niedfeldt, Brandon Carroll, and Joel Howard</p>"},{"location":"research/projects/tactical_seeability/#project-timeline","title":"Project Timeline","text":"<p>Phase 1: December 2008 - July 2009</p> <p>Phase 2: August 2009 - October 2011</p>"},{"location":"research/projects/tactical_seeability/#publications","title":"Publications","text":"<p>P. Niedfeldt, B. T. Carroll, R. W. Beard, J. Howard, B. Morse, and S. Pledgie. \"Enhanced UAS Surveillance Using a Video Utility Metric\". Under Review for International Journal on Unmanned Systems (World Scientific).</p> <p>P. Niedfeldt, B. T. Carroll, R. W. Beard, and S. Pledgie. \"A Staged Path Planner for an Unmanned Air System Performing Surveillance. AIAA 2012-4786, Proceedings of the 2012 AIAA Guidance, Navigation, and Control Conference, San Francisco, CA, USA.</p> <p>P. Niedfeldt, R. W. Beard, and S. Pledgie. \"Integrated Sensor Guidance Using Probability of Object Identification\". Proceedings of the 2010 American Control Conference, Baltimore, MD, USA. pp. 788-793.</p>"},{"location":"research/projects/tailsitter/","title":"Tailsitter","text":""},{"location":"research/projects/tailsitter/#tailsitter","title":"Tailsitter","text":"<p>A tailsitter is a fixed wing aircraft with vertical takeoff and landing (VTOL) capabilities but requires no additional moving parts as tilt-rotor, tilt-wing or vectored thrust aircraft do. This type of aircraft combines the flexibility of rotorcraft and endurance of fixed-wing aircraft. (See http://en.wikipedia.org/wiki/Tailsitter for more details.)</p> <p></p>"},{"location":"research/projects/tailsitter/#problem-statement","title":"Problem Statement","text":"<p>Autonomous flight desired from takeoff through landing, including transition between hover and level flight Platform must be robust to external disturbances such as wind during all flight regimes.</p>"},{"location":"research/projects/tailsitter/#challenges","title":"Challenges","text":"<p>Aircraft inherently unstable in hover Characterizing aerodynamics during transition Traditional methods of attitude description (Euler Angles) not sufficient Maintaining control authority while descending during hover</p>"},{"location":"research/projects/tailsitter/#goals","title":"Goals","text":"<p>Hardware-in-the-Loop Simulation o Interface Kestrel 3 Autopilot with the X-Plane flight simulator Control Algorithms o Implement previously developed control algorithms on Kestrel 3 Autopilot o Validate algorithm robustness to external disturbances, specifically during hover and landing o Improve transition controller: - Banking during transition - Minimize altitude gain while controlling horizontal position System Identification o Wind tunnel testing for general aircraft parameters o Flight Testing for aerodynamic and other physical parameter o Simulator Validation</p>"},{"location":"research/projects/tailsitter/#personnel","title":"Personnel","text":"<p>BYU: Dr. Randy Beard, Dr. Tim McLain, Matthew Argyle, Nathan Edwards, and Jason Beach</p> <p>MLB Company: Steve Morris www.spyplanes.com</p>"},{"location":"research/projects/tailsitter/#sponsor","title":"Sponsor","text":"<p>MLB was recently awarded a Phase II contract from the Air Force Research Lab. MLB has chosen BYU to develop the flight controls.</p>"},{"location":"research/projects/target_tracking/","title":"Target Tracking","text":""},{"location":"research/projects/target_tracking/#target-tracking","title":"Target Tracking","text":"<p>The Multiple Target Tracking (MTT) problem can be broken down into several subproblems:- Sensor processing- Data association between individual measurements and targets- Target state estimation- Autonomous path planning to position the UAS to best observe targets</p>"},{"location":"research/projects/target_tracking/#personnel","title":"Personnel","text":"<ul> <li>Peter Niedfeldt</li> <li>Randy Beard</li> </ul>"},{"location":"research/projects/target_tracking/#sponsor","title":"Sponsor","text":"<p>NDSEG Fellowship</p>"},{"location":"research/projects/target_tracking/#project-duration","title":"Project Duration","text":"<p>December 2011 - Present</p>"},{"location":"research/projects/target_tracking/#publications","title":"Publications","text":"<p>Preliminary work began in 2010 to estimate the position of a vehicle traveling on known road network using a Histogram Filter. A paper was published in the 2011 American Control Conference, which can be found here. The research for this paper was funded by the Air Force Research Laboratories, Air Vehicles Directorate.</p>"}]}